### 内容提要

* 本章主要讲解了web机器人一些原理和介绍，以及怎样控制机器人的访问和业界的一些关于跟踪机器人的规范，最后需要理解的一点就是机器人跟我们客户端一样遵守http规范，它是某种形式上的客户端。

#### 概念

* Web机器人是能够在无需人类干预的情况下自动进行一系列Web事务处理的软件程序。人们根据这些机器人探查web站点的方式，形象的给它们取了一个饱含特色的名字，比如“爬虫”、“蜘蛛”、“蠕虫”以及“机器人”等！

#### 爬虫及爬行方式

* “爬虫”主要采取的爬行方式是获取第一个web页面，然后递归地对各种信息性web站点进行遍历，从而获取相关页面。搜索引擎的爬虫是一些复杂的爬虫，因为他们不仅会爬行web页面，而且会把相关数据拉取回来建立数据库，方便用户搜索！

1. 爬虫会从根集开始爬行

2. 爬虫会解析页面所有的url，并把它们转换绝对形式

3. 要避免环路的出现，因为这些环路会暂停或减缓机器人的爬行过程

* 环路对爬虫有害的三个原因：

1. 爬虫会陷入循环之中，从而兜圈子，浪费带宽，无法获取新页面！

2. 爬虫无限的请求服务器，从而阻塞了真正的用户去请求服务器，这是可以作为法律诉讼理由的！

3. 爬虫服务器会被重复的数据充斥

* 网络中两个url表面上看起来不一样，但是指向的是同一资源，那么这两个url就互相称为“别名”，由于别名问题的存在，所以爬虫会爬行重复的数据，所以爬虫有必要把url的进行规范化！从而解决相关数据重复问题。相关规范方法如：没有端口默认为80，把字符转义为等价字符，删除#标签等。

#### 如何避免环路与重复

* 规范化URL：将URL转换为标准形式以避免语法上的别名

* 广度优先的爬行：每次爬虫都有大量潜在的URL要去爬行，如果实行广度URL优先爬行，那么即时碰到环路，机器人也可以回到环路中获取的下一个页面之前，如果采用深度优先方式，那么机器人很容易陷入环路，越陷越深。

* 节流：限制一段时间内机器人可以从一个web站点获取的页面数量。通过节流来限制重复的页面总数和对服务器的访问总数。

* 限制URL的大小：机器人可能会拒绝爬行超出特定长度（通常是1KB）的URL。如果环路使URL的长度增加，长度限制就会最终终止这个环路。但要小心，用此种技术肯定会让你错过一些内容，因为现在很多url都绑定了很多状态信息，所以一般情况下，它们长度都会很长。

* URL/站点黑名单：维护一个与机器人环路和陷阱相对应的已知站点及URL列表，然后像躲避瘟疫一样避开它们。发现新问题时，就将其加入黑名单。

* 模式检测：文件系统的符号连接和类似的错误配置所造成的环路会遵循某种模式，比如，URL会随着组件的复制逐渐增加。有些机器人会将具有重复组件的URL当作潜在的环路，拒绝爬行带有多于两或三个重复组件的URL。如形如“/dir/dir/dir...”格式的URL，那么机器人就怀疑它是潜在的环路，从而拒绝爬行。

* 内容指纹：说白了就是机器人对曾经爬行过的内容进行计算从而算出一个校验和，那么继续爬行其他页面时候，如果发现其它页面的“校验和”和前面算的相等，那么机器人就认为此内容已经获取过了，不需要进行重新获取。

* 人工监视：如题，人工检测，因为设计再好的机器人总是会陷入环路不能出来的时候，那么就需要人工进行干预，比如收集日志什么的。


#### 机器人的HTTP

* 相关首部

>> User-Agent ：机器人名字

>> From ：提供机器人管理者的E-mail地址

>> Accept : 告知服务器可以发送那些媒体类型

>> Referer ：提供包含了当前请求的URL的文档的URL

* 虚拟主机需要爬虫带Host首部，要不然会返回错误主机的数据

* 让爬虫使用条件请求是有意义的，因为有的数据内容没有改变，所以重复抓取是浪费空间的，只有在内容实际改变的时候才重新发起请求，即条件请求。

#### 行为不当的机器人

* 失控的机器人，比正常用户的请求速度快很多，当这类爬虫设计出现错误的时候，很容易短时间之内增加服务器的负载，阻止真正用户的访问，原因诸如：编程逻辑错误、陷入环路之中

* 失效的url，url可能已经失效了，但是爬虫依然取请求它 ，这样会让服务器的日志文档里面增加了很多请求出错的记录。

* 很长的错误url,同样请求这样一个url，会让服务器日志文档增加一个很杂论的出错记录

* 爱打听的机器人，访问了一些管理者不允许访问的内容，涉及侵犯隐私

* 动态网关访问


#### 拒绝机器人访问

* 通过一个叫robots.txt的文件来约束机器人的访问。它的思想就是指定那些部分机器人可以访问，那些部分机器人不能访问
。如果机器人遵循这个自愿约束标准，那么在请求所有资源之前，它需要获取robots.txt并解析它。

* 请求robots.txt时针对服务器返回的状态码，爬虫所作的动作：

>> 如果返回2xx代码，机器人就必须对内容进行解析，并使用排斥规则从那个站点上获取内容

>> 如果返回404，机器人认为服务器没有激活排斥规则，所以它不受限制


>> 如果返回401或403(访问限制)，表示机器人是完全受限的

>> 如果返回503（服务器临时故障），那么机器人暂时停止访问，知道正常之后继续请求robots.txt

>> 如果返回重定向代码，那么机器人也应该重定向到相关页面

* robots.txt文件的格式：包括三种内容注释行、空行、规则行。如：

``` javascript

	# this robots.txt file allows Slurp & Webcrawler to crawl
	# the public parts of our site,but no other robots...

	User-Agent: slurp
	User-Agent: webcraler
	Disallow: /private

	User-Agent: *
	Disallow:

```


#### 简单聊一下搜索引擎

* 搜索引擎是web机器人用得最多的领域

* 最初开始的搜索引擎就是一个简单的数据，哪里维护者一个用户可能搜索的信息列表。但如今的搜索引擎确实一个相当复杂的数据库，存储有大量信息，用了很多爬虫去爬取数据。所以对用户来说，搜索引擎的检索信息的速度以及爬虫获取数据的速度是搜索引擎需要必须考虑的问题。

* 全文检索就是一个数据库，给它一个单词，它可以立即提供包含那个单词的所有文档。创建了索引之后，就不需要对文档自身进行扫描了。

* 搜索引擎都有自己的排序算法，如相关性排名算法，如某个单词在很多内容都出现了，那么它的相关度就很高，所有与此单词有关的内容都应该排在考前。当然了这样的索引差不多在爬虫去获取内容的时候就已经建立起这种索引了！

